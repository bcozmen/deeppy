{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04ee211d-b15a-47f9-b503-d9df529e3f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import torch\n",
    "import numpy as np\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c46dc19c-d810-4678-913f-8ce19d97a9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    'hash_encoding': {\n",
    "        'num_levels': 16,\n",
    "        'level_dim': 2,\n",
    "        'input_dim': 3,\n",
    "        'log2_hashmap_size': 19,\n",
    "        'base_resolution': 16\n",
    "    },\n",
    "    'mlp': {\n",
    "        'num_layers': 3,  # Number of layers in geometric MLP\n",
    "        'hidden_dim': 64,  # Hidden dimension size\n",
    "    }\n",
    "}\n",
    "\n",
    "def load_torch_weights(file_path):\n",
    "    \"\"\"Load model weights from a checkpoint file.\"\"\"\n",
    "    try:\n",
    "        weights = torch.load(file_path, map_location='cpu')\n",
    "        return weights['model']\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading file {file_path}: {e}\")\n",
    "        return None\n",
    "        \n",
    "def extract_hash_encoding_structure(model_weights, num_levels=16, level_dim=2, input_dim=3, log2_hashmap_size=19, base_resolution=16):\n",
    "    \"\"\"\n",
    "    Extract and organize hash encoding weights into hierarchical structure.\n",
    "    \n",
    "    Args:\n",
    "        model_weights (dict): The loaded model weights dictionary\n",
    "        num_levels (int): Number of levels in hash encoding\n",
    "        level_dim (int): Dimension of encoding at each level\n",
    "        input_dim (int): Input dimension (typically 3 for 3D)\n",
    "        log2_hashmap_size (int): Log2 of maximum hash table size\n",
    "        base_resolution (int): Base resolution of the grid\n",
    "        \n",
    "    Returns:\n",
    "        dict: Hierarchical structure of hash encoding weights\n",
    "    \"\"\"\n",
    "    # Extract hash encoding embeddings\n",
    "    embeddings = model_weights['_orig_mod.grid_encoder.embeddings']\n",
    "    \n",
    "    # Calculate per-level parameters\n",
    "    max_params = 2 ** log2_hashmap_size\n",
    "    per_level_scale = np.exp2(np.log2(2048 / base_resolution) / (num_levels - 1))\n",
    "    \n",
    "    # Initialize structure to store weights\n",
    "    hash_structure = {}\n",
    "    offset = 0\n",
    "    \n",
    "    for level in range(num_levels):\n",
    "        # Calculate resolution at this level\n",
    "        resolution = int(np.ceil(base_resolution * (per_level_scale ** level)))\n",
    "        \n",
    "        # Calculate number of parameters for this level\n",
    "        params_in_level = min(max_params, (resolution) ** input_dim)\n",
    "        params_in_level = int(np.ceil(params_in_level / 8) * 8)  # make divisible by 8\n",
    "        \n",
    "        # Extract weights for this level\n",
    "        level_weights = embeddings[offset:offset + params_in_level]\n",
    "        \n",
    "        # Store level information\n",
    "        hash_structure[f'level_{level}'] = {\n",
    "            'resolution': resolution,\n",
    "            'num_params': params_in_level,\n",
    "            'weights': level_weights,\n",
    "            'weights_shape': level_weights.shape,\n",
    "            'scale': per_level_scale ** level\n",
    "        }\n",
    "        \n",
    "        offset += params_in_level\n",
    "    \n",
    "    # Add global information\n",
    "    hash_structure['global_info'] = {\n",
    "        'total_params': offset,\n",
    "        'embedding_dim': level_dim,\n",
    "        'base_resolution': base_resolution,\n",
    "        'max_resolution': int(np.ceil(base_resolution * (per_level_scale ** (num_levels-1)))),\n",
    "        'per_level_scale': per_level_scale\n",
    "    }\n",
    "    \n",
    "    return hash_structure\n",
    "\n",
    "def extract_mlp_weights(model_weights):\n",
    "    \"\"\"Extract geometric and view-dependent MLP weights from the model.\"\"\"\n",
    "    geometry_layers = {}\n",
    "    view_mlp_layers = {}\n",
    "    \n",
    "    # Extract geometry MLP weights\n",
    "    for i in range(CONFIG['mlp']['num_layers']):\n",
    "        weight_key = f'_orig_mod.grid_mlp.net.{i}.weight'\n",
    "        bias_key = f'_orig_mod.grid_mlp.net.{i}.bias'\n",
    "        \n",
    "        if weight_key in model_weights:\n",
    "            geometry_layers[f'layer_{i}'] = {\n",
    "                'weights': model_weights[weight_key],\n",
    "                'shape': model_weights[weight_key].shape\n",
    "            }\n",
    "            \n",
    "            if bias_key in model_weights:\n",
    "                geometry_layers[f'layer_{i}']['bias'] = model_weights[bias_key]\n",
    "    \n",
    "    # Extract view-dependent MLP weights\n",
    "    for i in range(CONFIG['mlp']['num_layers']):\n",
    "        weight_key = f'_orig_mod.view_mlp.net.{i}.weight'\n",
    "        bias_key = f'_orig_mod.view_mlp.net.{i}.bias'\n",
    "        \n",
    "        if weight_key in model_weights:\n",
    "            view_mlp_layers[f'layer_{i}'] = {\n",
    "                'weights': model_weights[weight_key],\n",
    "                'shape': model_weights[weight_key].shape\n",
    "            }\n",
    "            \n",
    "            if bias_key in model_weights:\n",
    "                view_mlp_layers[f'layer_{i}']['bias'] = model_weights[bias_key]\n",
    "    \n",
    "    return {\n",
    "        'geometry_mlp': geometry_layers,\n",
    "        'view_mlp': view_mlp_layers\n",
    "    }\n",
    "\n",
    "# Example usage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "921aa6ad-c744-4d74-b762-823edac97ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../../ten_objs/ten_objs/shared_data/CarrotKhanStatue/base_000_000_000/checkpoints\"\n",
    "nerf = load_torch_weights(data_path + \"/final.pth\")\n",
    "mlp_weights = extract_mlp_weights(nerf)\n",
    "mrhe_by_layer = extract_hash_encoding_structure(nerf)\n",
    "\n",
    "del mrhe_by_layer[\"global_info\"]\n",
    "\n",
    "\n",
    "for key,value in mlp_weights[\"geometry_mlp\"].items():\n",
    "    new_key = \"geo_\" + key\n",
    "    mrhe_by_layer[new_key] = value\n",
    "\n",
    "for key,value in mlp_weights[\"view_mlp\"].items():\n",
    "    new_key = \"view_\" + key\n",
    "    mrhe_by_layer[new_key] = value\n",
    "\n",
    "with h5py.File(data_path + \"/final.h5\", \"w\") as h5f:\n",
    "    for name, tensor in mrhe_by_layer.items():\n",
    "        h5f.create_dataset(name, data=tensor[\"weights\"].numpy(), compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "241c44d3-7cee-42eb-9b53-431626ae800c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../../../../../..//media/boz/408422C88422C070/ada_and_hal/1Story/base_000_000_000/checkpoints/final.h5\n",
      "../../../../../../..//media/boz/408422C88422C070/ada_and_hal/1Story/base_000_000_000/checkpoints\n",
      "../../../../../../..//media/boz/408422C88422C070/ada_and_hal/1Story/compound_090_000_090/checkpoints/final.h5\n",
      "../../../../../../..//media/boz/408422C88422C070/ada_and_hal/1Story/compound_090_000_090/checkpoints\n",
      "../../../../../../..//media/boz/408422C88422C070/ada_and_hal/1Story/x_180_000_000/checkpoints/final.h5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m h5py\u001b[38;5;241m.\u001b[39mFile(data_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/final.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m h5f:\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name, tensor \u001b[38;5;129;01min\u001b[39;00m mrhe_by_layer\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m---> 28\u001b[0m         h5f\u001b[38;5;241m.\u001b[39mcreate_dataset(name, data\u001b[38;5;241m=\u001b[39mtensor[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweights\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mnumpy(), compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgzip\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(data_path)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ix \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m8\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/deeppy/lib/python3.13/site-packages/h5py/_hl/group.py:186\u001b[0m, in \u001b[0;36mGroup.create_dataset\u001b[0;34m(self, name, shape, dtype, data, **kwds)\u001b[0m\n\u001b[1;32m    183\u001b[0m         parent_path, name \u001b[38;5;241m=\u001b[39m name\u001b[38;5;241m.\u001b[39mrsplit(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    184\u001b[0m         group \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequire_group(parent_path)\n\u001b[0;32m--> 186\u001b[0m dsid \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mmake_new_dset(group, shape, dtype, data, name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    187\u001b[0m dset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mDataset(dsid)\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dset\n",
      "File \u001b[0;32m~/anaconda3/envs/deeppy/lib/python3.13/site-packages/h5py/_hl/dataset.py:178\u001b[0m, in \u001b[0;36mmake_new_dset\u001b[0;34m(parent, shape, dtype, data, name, chunks, compression, shuffle, fletcher32, maxshape, compression_opts, fillvalue, scaleoffset, track_times, external, track_order, dcpl, dapl, efile_prefix, virtual_prefix, allow_unknown_filter, rdcc_nslots, rdcc_nbytes, rdcc_w0, fill_time)\u001b[0m\n\u001b[1;32m    175\u001b[0m dset_id \u001b[38;5;241m=\u001b[39m h5d\u001b[38;5;241m.\u001b[39mcreate(parent\u001b[38;5;241m.\u001b[39mid, name, tid, sid, dcpl\u001b[38;5;241m=\u001b[39mdcpl, dapl\u001b[38;5;241m=\u001b[39mdapl)\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, Empty)):\n\u001b[0;32m--> 178\u001b[0m     dset_id\u001b[38;5;241m.\u001b[39mwrite(h5s\u001b[38;5;241m.\u001b[39mALL, h5s\u001b[38;5;241m.\u001b[39mALL, data)\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dset_id\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "objects = glob.glob(\"../../../../../../..//media/boz/408422C88422C070/ada_and_hal/*\")\n",
    "files = []\n",
    "for obj in objects:\n",
    "    files += glob.glob(obj +  \"/*/checkpoints/*.pth\")\n",
    "\n",
    "files.sort()\n",
    "\n",
    "for ix,data_path in enumerate(files):\n",
    "    data_path = data_path.replace(\"/final.pth\", \"\")\n",
    "    nerf = load_torch_weights(data_path + \"/final.pth\")\n",
    "    mlp_weights = extract_mlp_weights(nerf)\n",
    "    mrhe_by_layer = extract_hash_encoding_structure(nerf)\n",
    "    \n",
    "    del mrhe_by_layer[\"global_info\"]\n",
    "    \n",
    "    \n",
    "    for key,value in mlp_weights[\"geometry_mlp\"].items():\n",
    "        new_key = \"geo_\" + key\n",
    "        mrhe_by_layer[new_key] = value\n",
    "    \n",
    "    for key,value in mlp_weights[\"view_mlp\"].items():\n",
    "        new_key = \"view_\" + key\n",
    "        mrhe_by_layer[new_key] = value\n",
    "\n",
    "    print(data_path + \"/final.h5\")\n",
    "    with h5py.File(data_path + \"/final.h5\", \"w\") as h5f:\n",
    "        for name, tensor in mrhe_by_layer.items():\n",
    "            h5f.create_dataset(name, data=tensor[\"weights\"].numpy(), compression=\"gzip\")\n",
    "    print(data_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cb9318a9-6291-483d-a257-baad2b6602c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
