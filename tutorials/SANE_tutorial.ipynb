{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45704f5f-da5d-403d-8834-cea4bed271a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the path to the parent module\n",
    "sys.path.append(os.path.abspath('../..'))\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "import deeppy as dp\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tiktoken\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "from deeppy import LearnFrame,LayerGenerator,FromLoader\n",
    "from deeppy import Network\n",
    "from deeppy.models.cv import Sane\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50f6ba7f-4b5f-4aaf-bbf3-90cefd1b3460",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "vocab_size = 200\n",
    "embed_dim = 1024\n",
    "latent_dim = 128\n",
    "num_heads = 4\n",
    "num_layers = 4\n",
    "context_size = 50\n",
    "dropout = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "471c307d-c124-47b9-99ff-11c24f5998be",
   "metadata": {},
   "outputs": [],
   "source": [
    "Optimizer_params = {\n",
    "    \"optimizer\":optim.AdamW,\n",
    "    \"optimizer_args\":{\"lr\":3e-4, \"amsgrad\" : True},\n",
    "    \"clipper\":nn.utils.clip_grad_norm_,\n",
    "    \"clipper_params\":{\"max_norm\" : 1.0},\n",
    "    \"scheduler_params\":None,\n",
    "}\n",
    "\n",
    "Sane_params = {\n",
    "    \"optimizer_params\":Optimizer_params,\n",
    "    \"vocab_size\":vocab_size,\n",
    "    \"embed_dim\":embed_dim,\n",
    "    \"latent_dim\":latent_dim,\n",
    "    \"num_heads\":num_heads,\n",
    "    \"num_layers\":num_layers,\n",
    "    \"context_size\":context_size,\n",
    "    \"dropout\":dropout,\n",
    "    \"device\":device,\n",
    "    \"criterion\":nn.MSELoss(),\n",
    "}\n",
    "\n",
    "model = dp.cv.Sane(**Sane_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c90139ec-afc7-440f-a17b-b63487e70a19",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Network(\n",
       "  (model): Sequential(\n",
       "    (0): Linear(in_features=200, out_features=1022, bias=True)\n",
       "    (1): DummyPositionalEmbedding()\n",
       "    (2): Dropout(p=0.1, inplace=False)\n",
       "    (3): MaskedTransformerEncoder(\n",
       "      (encoder): TransformerEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0-3): 4 x TransformerEncoderLayer(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (linear1): Linear(in_features=1024, out_features=2048, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (linear2): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout1): Dropout(p=0.1, inplace=False)\n",
       "            (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (4): Linear(in_features=1024, out_features=128, bias=True)\n",
       "    (5): Linear(in_features=128, out_features=1022, bias=True)\n",
       "    (6): DummyPositionalEmbedding()\n",
       "    (7): Dropout(p=0.1, inplace=False)\n",
       "    (8): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-3): 4 x TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=1024, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (9): Linear(in_features=1024, out_features=200, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2f06604-5bce-4304-8702-6e39de5124a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assume that layer of a NN is already flattened and the following tensor\n",
      " is batch_size x cout x cr\n",
      "torch.Size([64, 10, 200])\n"
     ]
    }
   ],
   "source": [
    "cout = 10\n",
    "cr = vocab_size\n",
    "tokenized_input = torch.rand(size = (batch_size, cout, cr))\n",
    "\n",
    "print(\"Assume that layer of a NN is already flattened and the following tensor\\n is batch_size x cout x cr\")\n",
    "print(f\"{tokenized_input.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22e61cc-0262-48ee-8322-b7ab386a3ce1",
   "metadata": {},
   "source": [
    "# Encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fbf9870c-2903-48b3-be88-a1f27dc042a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latent space : torch.Size([64, 10, 128])\n"
     ]
    }
   ],
   "source": [
    "latent = model.encode(tokenized_input)\n",
    "print(f\"Latent space : {latent.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "19bb2c34-7ae7-4bc6-b314-a7de7863fc2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized input : torch.Size([64, 10, 1022])\n",
      "Position encoding + dropout : torch.Size([64, 10, 1024])\n",
      "After transformer encoder : torch.Size([64, 10, 1024])\n",
      "Latent space : torch.Size([64, 10, 128])\n"
     ]
    }
   ],
   "source": [
    "T = model.net.model[0](tokenized_input)\n",
    "print(f\"Tokenized input : {T.shape}\")\n",
    "\n",
    "Tp = model.net.model[1:3](T)\n",
    "print(f\"Position encoding + dropout : {Tp.shape}\")\n",
    "\n",
    "Tr = model.net.model[3](Tp)\n",
    "print(f\"After transformer encoder : {Tr.shape}\")\n",
    "\n",
    "latent = model.net.model[4](Tp)\n",
    "print(f\"Latent space : {latent.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4e4679-9089-439d-838a-db408b6cd952",
   "metadata": {},
   "source": [
    "# Decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3ad68be0-1580-40bd-ba90-2120737a992c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output : torch.Size([64, 10, 200])\n"
     ]
    }
   ],
   "source": [
    "z = model.decode(latent)\n",
    "print(f\"Output : {z.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d09b4233-d45d-484b-af35-1be5d73afd2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder compression : torch.Size([64, 10, 1022])\n",
      "Decoder position encoding + dropout : torch.Size([64, 10, 1024])\n",
      "Decoder transformer : torch.Size([64, 10, 1024])\n",
      "Output : torch.Size([64, 10, 200])\n"
     ]
    }
   ],
   "source": [
    "T = model.net.model[5](latent)\n",
    "print(f\"Decoder compression : {T.shape}\")\n",
    "\n",
    "T = model.net.model[6:7](T)\n",
    "print(f\"Decoder position encoding + dropout : {T.shape}\")\n",
    "\n",
    "T = model.net.model[8](T)\n",
    "print(f\"Decoder transformer : {T.shape}\")\n",
    "\n",
    "z = model.net.model[9](T)\n",
    "print(f\"Output : {z.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d2ec6b-f859-40c1-8171-5b4a1a1dbc63",
   "metadata": {},
   "source": [
    "# Autoencoder Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "36a1e326-122c-4b14-b072-c243548f681d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output : torch.Size([64, 10, 200])\n"
     ]
    }
   ],
   "source": [
    "z = model(tokenized_input)\n",
    "print(f\"Output : {z.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
