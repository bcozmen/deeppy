{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45704f5f-da5d-403d-8834-cea4bed271a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the path to the parent module\n",
    "sys.path.append(os.path.abspath('../..'))\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "import deeppy as dp\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tiktoken\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "from deeppy import LearnFrame,LayerGenerator,FromLoader\n",
    "from deeppy import Network\n",
    "from deeppy.models.cv import Sane\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0023648a-cd4b-4c08-a253-e3ea553bda2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        arch_params = {\n",
      "            \"layers\":[],\n",
      "            \"blocks\":[],\n",
      "            \"block_args\":[],\n",
      "            \"out_act\":<class 'torch.nn.modules.linear.Identity'>,\n",
      "            \"out_params\":{},\n",
      "            \"weight_init\":None,\n",
      "        }\n",
      "            Scheduler_params = {\n",
      "                \"scheduler\",\n",
      "                \"auto_step\":True,\n",
      "                \"**kwargs\",\n",
      "            }\n",
      "        Optimizer_params = {\n",
      "            \"configure_optimizer\":None,\n",
      "            \"optimizer\":<class 'torch.optim.adamw.AdamW'>,\n",
      "            \"optimizer_args\":{},\n",
      "            \"clipper\":None,\n",
      "            \"clipper_params\":{},\n",
      "            \"scheduler_params\":None,\n",
      "        }\n",
      "    Network_params = {\n",
      "        \"arch_params\",\n",
      "        \"decoder_params\":None,\n",
      "        \"task\":'reg',\n",
      "        \"optimizer_params\":None,\n",
      "        \"torch_compile\":False,\n",
      "    }\n",
      "Sane_params = {\n",
      "    \"optimizer_params\",\n",
      "    \"max_positions\",\n",
      "    \"input_dim\":201,\n",
      "    \"latent_dim\":128,\n",
      "    \"projection_dim\":30,\n",
      "    \"embed_dim\":1024,\n",
      "    \"num_heads\":4,\n",
      "    \"num_layers\":4,\n",
      "    \"dropout\":0.1,\n",
      "    \"context_size\":50,\n",
      "    \"bias\":True,\n",
      "    \"gamma\":0.5,\n",
      "    \"ntx_temp\":0.1,\n",
      "    \"device\":None,\n",
      "    \"amp\":False,\n",
      "    \"torch_compile\":False,\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "Sane.print_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50f6ba7f-4b5f-4aaf-bbf3-90cefd1b3460",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "input_dim = 20\n",
    "embed_dim = 64\n",
    "latent_dim = 128\n",
    "num_heads = 4\n",
    "num_layers = 4\n",
    "window_size = 10\n",
    "dropout = 0.1\n",
    "bias = False\n",
    "projection_dim = 10\n",
    "\n",
    "one_epoch_length = 1000\n",
    "epochs = 50 * one_epoch_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "471c307d-c124-47b9-99ff-11c24f5998be",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler_config = {\n",
    "   \n",
    "}\n",
    "\n",
    "Scheduler_params = {\n",
    "                \"scheduler\" : optim.lr_scheduler.OneCycleLR,\n",
    "                \"auto_step\":True,\n",
    "                 \"max_lr\": 3e-4,\n",
    "                \"total_steps\": epochs,\n",
    "                \"pct_start\": 0.3,\n",
    "                \"anneal_strategy\": \"cos\",\n",
    "                \"cycle_momentum\": True,\n",
    "                \"base_momentum\": 0.85,\n",
    "                \"max_momentum\": 0.95,\n",
    "                \"div_factor\": 25.0,\n",
    "                \"final_div_factor\": 10000.0,\n",
    "                \"three_phase\": False,\n",
    "                \"last_epoch\": -1,\n",
    "                \"verbose\": False,\n",
    "}\n",
    "\n",
    "Optimizer_params = {\n",
    "    \"optimizer\":optim.AdamW,\n",
    "    \"optimizer_args\":{\"lr\":3e-4, \"amsgrad\" : True, \"weight_decay\" : 3e-4, \"fused\" : True},\n",
    "    \"clipper\":nn.utils.clip_grad_norm_,\n",
    "    \"clipper_params\":{\"max_norm\" : 500.0},\n",
    "    \"scheduler_params\":Scheduler_params,\n",
    "}\n",
    "\n",
    "Sane_params = {\n",
    "    \"optimizer_params\":Optimizer_params,\n",
    "    \"max_positions\" : [500,500,500],\n",
    "    \"input_dim\":input_dim,\n",
    "    \"latent_dim\":latent_dim,\n",
    "    \"projection_dim\" : projection_dim,\n",
    "    \"embed_dim\":embed_dim,\n",
    "    \"num_heads\":num_heads,\n",
    "    \"num_layers\":num_layers,\n",
    "    \"context_size\":window_size,\n",
    "    \"dropout\":dropout,\n",
    "    \"bias\" : bias,\n",
    "    \"device\":device,\n",
    "    \"gamma\" : 0.5,\n",
    "    \"ntx_temp\" : 0.1,\n",
    "    \"torch_compile\" : False,\n",
    "    \"amp\":False,\n",
    "\n",
    "}\n",
    "\n",
    "model = dp.cv.Sane(**Sane_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154e0c35-663b-4d24-b00d-7d28268ee1c1",
   "metadata": {},
   "source": [
    "# Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d56010bb-1512-4837-b2a0-c6e33b252195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assume that layer of a NN is already flattened and the following tensor\n",
      " is batch_size x cout x cr\n",
      "Inp shape : torch.Size([64, 10, 20])\n",
      "mask shape : torch.Size([256, 10, 10])\n",
      "positions shape : torch.Size([64, 10, 3])\n"
     ]
    }
   ],
   "source": [
    "cout = window_size\n",
    "cr = input_dim\n",
    "\n",
    "tokenized_input = torch.rand(size = (batch_size, cout, cr)).to(device)\n",
    "mask = torch.log(torch.randint(0,2,size = (batch_size*num_heads, cout, cout))).to(device)\n",
    "positions = torch.randint(0,500, size = (batch_size,cout,3)).to(device)\n",
    "\n",
    "tokenized_input2 = torch.rand(size = (batch_size, cout, cr)).to(device)\n",
    "mask2 = torch.log(torch.randint(0,2,size = (batch_size*num_heads, cout, cout))).to(device)\n",
    "positions2 = torch.randint(0,500, size = (batch_size,cout,3)).to(device)\n",
    "\n",
    "print(\"Assume that layer of a NN is already flattened and the following tensor\\n is batch_size x cout x cr\")\n",
    "\n",
    "\n",
    "print(f\"Inp shape : {tokenized_input.shape}\")\n",
    "print(f\"mask shape : {mask.shape}\")\n",
    "print(f\"positions shape : {positions.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62627348-6290-4c12-aeec-bdcfdb79a2e9",
   "metadata": {},
   "source": [
    "# Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c90139ec-afc7-440f-a17b-b63487e70a19",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Network(\n",
       "  (model): Sequential(\n",
       "    (0): LinearBeforePosition(\n",
       "      (linear): Linear(in_features=20, out_features=64, bias=True)\n",
       "    )\n",
       "    (1): SanePositionalEmbedding(\n",
       "      (pe1): Embedding(500, 32)\n",
       "      (pe2): Embedding(500, 32)\n",
       "      (pe3): Embedding(500, 32)\n",
       "    )\n",
       "    (2): Dropout(p=0.1, inplace=False)\n",
       "    (3): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-3): 4 x TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=False)\n",
       "          )\n",
       "          (linear1): Linear(in_features=64, out_features=256, bias=False)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=256, out_features=64, bias=False)\n",
       "          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (activation): GELU(approximate='none')\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (4): Linear(in_features=64, out_features=128, bias=True)\n",
       "    (5): LinearBeforePosition(\n",
       "      (linear): Linear(in_features=128, out_features=64, bias=True)\n",
       "    )\n",
       "    (6): SanePositionalEmbedding(\n",
       "      (pe1): Embedding(500, 32)\n",
       "      (pe2): Embedding(500, 32)\n",
       "      (pe3): Embedding(500, 32)\n",
       "    )\n",
       "    (7): Dropout(p=0.1, inplace=False)\n",
       "    (8): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-3): 4 x TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=False)\n",
       "          )\n",
       "          (linear1): Linear(in_features=64, out_features=256, bias=False)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=256, out_features=64, bias=False)\n",
       "          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (activation): GELU(approximate='none')\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (9): Linear(in_features=64, out_features=20, bias=True)\n",
       "  )\n",
       "  (encode): Sequential(\n",
       "    (0): LinearBeforePosition(\n",
       "      (linear): Linear(in_features=20, out_features=64, bias=True)\n",
       "    )\n",
       "    (1): SanePositionalEmbedding(\n",
       "      (pe1): Embedding(500, 32)\n",
       "      (pe2): Embedding(500, 32)\n",
       "      (pe3): Embedding(500, 32)\n",
       "    )\n",
       "    (2): Dropout(p=0.1, inplace=False)\n",
       "    (3): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-3): 4 x TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=False)\n",
       "          )\n",
       "          (linear1): Linear(in_features=64, out_features=256, bias=False)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=256, out_features=64, bias=False)\n",
       "          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (activation): GELU(approximate='none')\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (4): Linear(in_features=64, out_features=128, bias=True)\n",
       "  )\n",
       "  (decode): Sequential(\n",
       "    (5): LinearBeforePosition(\n",
       "      (linear): Linear(in_features=128, out_features=64, bias=True)\n",
       "    )\n",
       "    (6): SanePositionalEmbedding(\n",
       "      (pe1): Embedding(500, 32)\n",
       "      (pe2): Embedding(500, 32)\n",
       "      (pe3): Embedding(500, 32)\n",
       "    )\n",
       "    (7): Dropout(p=0.1, inplace=False)\n",
       "    (8): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-3): 4 x TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=False)\n",
       "          )\n",
       "          (linear1): Linear(in_features=64, out_features=256, bias=False)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=256, out_features=64, bias=False)\n",
       "          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (activation): GELU(approximate='none')\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (9): Linear(in_features=64, out_features=20, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22e61cc-0262-48ee-8322-b7ab386a3ce1",
   "metadata": {},
   "source": [
    "## Encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fbf9870c-2903-48b3-be88-a1f27dc042a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latent space : torch.Size([64, 10, 128])\n"
     ]
    }
   ],
   "source": [
    "latent = model.encode((tokenized_input,positions))\n",
    "print(f\"Latent space : {latent.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19bb2c34-7ae7-4bc6-b314-a7de7863fc2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized input : torch.Size([64, 10, 64])\n",
      "Position encoding + dropout : torch.Size([64, 10, 64])\n",
      "After transformer encoder : torch.Size([64, 10, 64])\n",
      "Latent space : torch.Size([64, 10, 128])\n"
     ]
    }
   ],
   "source": [
    "T = model.autoencoder.model[0]((tokenized_input,positions))\n",
    "print(f\"Tokenized input : {T[0].shape}\")\n",
    "\n",
    "T = model.autoencoder.model[1](T)\n",
    "T = model.autoencoder.model[2](T)\n",
    "print(f\"Position encoding + dropout : {T.shape}\")\n",
    "\n",
    "T = model.autoencoder.model[3](T)\n",
    "print(f\"After transformer encoder : {T.shape}\")\n",
    "\n",
    "latent = model.autoencoder.model[4](T)\n",
    "print(f\"Latent space : {latent.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4e4679-9089-439d-838a-db408b6cd952",
   "metadata": {},
   "source": [
    "## Decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ad68be0-1580-40bd-ba90-2120737a992c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output : torch.Size([64, 10, 20])\n"
     ]
    }
   ],
   "source": [
    "z = model.decode((latent,positions))\n",
    "print(f\"Output : {z.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d09b4233-d45d-484b-af35-1be5d73afd2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder compression : torch.Size([64, 10, 64])\n",
      "Decoder position encoding + dropout : torch.Size([64, 10, 64])\n",
      "Decoder transformer : torch.Size([64, 10, 64])\n",
      "Output : torch.Size([64, 10, 20])\n"
     ]
    }
   ],
   "source": [
    "T = model.autoencoder.model[5]((latent,positions))\n",
    "print(f\"Decoder compression : {T[0].shape}\")\n",
    "\n",
    "T = model.autoencoder.model[6](T)\n",
    "T = model.autoencoder.model[7](T)\n",
    "print(f\"Decoder position encoding + dropout : {T.shape}\")\n",
    "\n",
    "T = model.autoencoder.model[8](T)\n",
    "print(f\"Decoder transformer : {T.shape}\")\n",
    "\n",
    "z = model.autoencoder.model[9](T)\n",
    "print(f\"Output : {z.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d2ec6b-f859-40c1-8171-5b4a1a1dbc63",
   "metadata": {},
   "source": [
    "## Autoencoder Full Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36a1e326-122c-4b14-b072-c243548f681d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output : torch.Size([64, 10, 20])\n"
     ]
    }
   ],
   "source": [
    "z,y, zp = model((tokenized_input,positions))\n",
    "print(f\"Output : {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a811d5-a94c-4732-928b-c391941a11f5",
   "metadata": {},
   "source": [
    "# Projection Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae9e8a07-ca60-4bb7-8f71-047511dc2871",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Network(\n",
       "  (model): Sequential(\n",
       "    (0): SqueezeLastDimention()\n",
       "    (1): Linear(in_features=1280, out_features=10, bias=False)\n",
       "    (2): LayerNorm((10,), eps=1e-05, elementwise_affine=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=10, out_features=10, bias=False)\n",
       "    (5): LayerNorm((10,), eps=1e-05, elementwise_affine=True)\n",
       "    (6): ReLU()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a67a25a1-b617-4485-9a06-c7abf472188f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latent size : torch.Size([64, 10, 128])\n",
      "Projection head output size : torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "latent = model.encode((tokenized_input,positions))\n",
    "p = model.project(latent)\n",
    "print(f\"Latent size : {latent.shape}\")\n",
    "print(f\"Projection head output size : {p.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b5a9b8-d3ca-476e-a132-fb7b33e5bc27",
   "metadata": {},
   "source": [
    "# Train a Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b4ed7c9-735d-47fa-84ac-d0a69a9456c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.375505447387695\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "st= time.time()\n",
    "for i in range(100):\n",
    "    mask = torch.randint(0,2, size = tokenized_input.shape)\n",
    "    mask2 = torch.randint(0,2, size = tokenized_input.shape)\n",
    "    \n",
    "    model.train()\n",
    "    batch = (tokenized_input,positions,mask,tokenized_input2,positions2,mask2)\n",
    "    loss = model.optimize(batch)\n",
    "print(time.time()-st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddd5cb6-72cd-447f-8e21-7988d144bc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#AMP Enabled  39.35s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85323026-cec9-4f5f-bd52-6eca95c72674",
   "metadata": {},
   "source": [
    "# Test a Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2ea6dab8-d706-460c-8cec-dd1661dc884c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss train : 2.7044646739959717\n"
     ]
    }
   ],
   "source": [
    "mask = torch.randint(0,2, size = tokenized_input.shape)\n",
    "mask2 = torch.randint(0,2, size = tokenized_input.shape)\n",
    "\n",
    "model.eval()\n",
    "batch = (tokenized_input,positions,mask,tokenized_input2,positions2,mask2)\n",
    "loss = model.test(batch)\n",
    "print(f\"Loss train : {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc58981-780c-48fe-a8ea-2fe08548dd8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1d014d7a-f2c2-484b-8504-680f66cd8daa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0884, -0.0852,  0.0957, -0.0459,  0.1008,  0.0480,  0.0048,  0.0048,\n",
       "         0.0518, -0.0283,  0.1175,  0.0478, -0.0952,  0.0461, -0.0631, -0.0968,\n",
       "         0.0749, -0.0668, -0.0130, -0.0731, -0.0374, -0.1216,  0.0615,  0.0310,\n",
       "         0.1175,  0.0251,  0.0499,  0.1142, -0.0383, -0.0044,  0.0230,  0.0812,\n",
       "         0.1186,  0.0520, -0.0166, -0.0265, -0.1085, -0.1184,  0.1093,  0.0712,\n",
       "         0.0804, -0.0976, -0.0139,  0.1037, -0.0398, -0.1014, -0.0141, -0.0309,\n",
       "         0.0296,  0.0385, -0.1172, -0.0913,  0.1184,  0.0628,  0.0293, -0.1128,\n",
       "         0.0141,  0.0807,  0.0814, -0.0591, -0.1241, -0.0770,  0.0162,  0.1070],\n",
       "       device='cuda:0', grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.autoencoder.model[-1].weight[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f0dd2a58-c728-4ddc-ad5f-1a32bff56a00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0884, -0.0852,  0.0957, -0.0459,  0.1008,  0.0480,  0.0048,  0.0048,\n",
       "         0.0518, -0.0283,  0.1175,  0.0478, -0.0952,  0.0461, -0.0631, -0.0968,\n",
       "         0.0749, -0.0668, -0.0130, -0.0731, -0.0374, -0.1216,  0.0615,  0.0310,\n",
       "         0.1175,  0.0251,  0.0499,  0.1142, -0.0383, -0.0044,  0.0230,  0.0812,\n",
       "         0.1186,  0.0520, -0.0166, -0.0265, -0.1085, -0.1184,  0.1093,  0.0712,\n",
       "         0.0804, -0.0976, -0.0139,  0.1037, -0.0398, -0.1014, -0.0141, -0.0309,\n",
       "         0.0296,  0.0385, -0.1172, -0.0913,  0.1184,  0.0628,  0.0293, -0.1128,\n",
       "         0.0141,  0.0807,  0.0814, -0.0591, -0.1241, -0.0770,  0.0162,  0.1070],\n",
       "       device='cuda:0', grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.autoencoder.decode[-1].weight[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ea107386-2786-470d-9304-87a31d67f819",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = model.autoencoder.save_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d97ad98d-93b9-45ef-9594-2a3fcca72472",
   "metadata": {},
   "outputs": [],
   "source": [
    "a[\"net\"][\"0.linear.weight\"][0][0] = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1b8a0515-97d1-474d-bd56-bec53e5b26f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.autoencoder.load_states(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "89f29df3-4eb2-40b1-8f77-357974153706",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 5.0000,  0.2051, -0.2166,  ..., -0.0506,  0.0625,  0.0721],\n",
       "        [-0.1544, -0.0997,  0.1204,  ...,  0.0361, -0.1982, -0.1364],\n",
       "        [ 0.0782,  0.2102,  0.0361,  ...,  0.1934, -0.1516, -0.1503],\n",
       "        ...,\n",
       "        [-0.0274, -0.1634, -0.1556,  ...,  0.1715,  0.0602,  0.0564],\n",
       "        [ 0.0221, -0.0697, -0.1722,  ...,  0.1954, -0.0897,  0.2192],\n",
       "        [-0.1845,  0.1456, -0.0325,  ...,  0.1797,  0.0090, -0.0244]],\n",
       "       device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.autoencoder.model[0].linear.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eff1a382-f5c1-43e4-986d-11a7ad5d4b16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 5.0000,  0.2051, -0.2166,  ..., -0.0506,  0.0625,  0.0721],\n",
       "        [-0.1544, -0.0997,  0.1204,  ...,  0.0361, -0.1982, -0.1364],\n",
       "        [ 0.0782,  0.2102,  0.0361,  ...,  0.1934, -0.1516, -0.1503],\n",
       "        ...,\n",
       "        [-0.0274, -0.1634, -0.1556,  ...,  0.1715,  0.0602,  0.0564],\n",
       "        [ 0.0221, -0.0697, -0.1722,  ...,  0.1954, -0.0897,  0.2192],\n",
       "        [-0.1845,  0.1456, -0.0325,  ...,  0.1797,  0.0090, -0.0244]],\n",
       "       device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.autoencoder.encode[0].linear.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0f2b29-747e-4466-a0d9-2dfa744fad3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
