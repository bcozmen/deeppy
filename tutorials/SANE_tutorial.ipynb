{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45704f5f-da5d-403d-8834-cea4bed271a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the path to the parent module\n",
    "sys.path.append(os.path.abspath('../..'))\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "import deeppy as dp\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tiktoken\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "from deeppy import LearnFrame,LayerGenerator,FromLoader\n",
    "from deeppy import Network\n",
    "from deeppy.models.cv import Sane\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0023648a-cd4b-4c08-a253-e3ea553bda2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        arch_params = {\n",
      "            \"layers\":[],\n",
      "            \"blocks\":[],\n",
      "            \"block_args\":[],\n",
      "            \"out_act\":<class 'torch.nn.modules.linear.Identity'>,\n",
      "            \"out_params\":{},\n",
      "            \"weight_init\":None,\n",
      "        }\n",
      "            Scheduler_params = {\n",
      "                \"scheduler\",\n",
      "                \"auto_step\":True,\n",
      "                \"**kwargs\",\n",
      "            }\n",
      "        Optimizer_params = {\n",
      "            \"configure_optimizer\":None,\n",
      "            \"optimizer\":<class 'torch.optim.adamw.AdamW'>,\n",
      "            \"optimizer_args\":{},\n",
      "            \"clipper\":None,\n",
      "            \"clipper_params\":{},\n",
      "            \"scheduler_params\":None,\n",
      "        }\n",
      "    Network_params = {\n",
      "        \"arch_params\",\n",
      "        \"decoder_params\":None,\n",
      "        \"task\":'reg',\n",
      "        \"optimizer_params\":None,\n",
      "    }\n",
      "Sane_params = {\n",
      "    \"optimizer_params\",\n",
      "    \"max_positions\",\n",
      "    \"input_dim\":201,\n",
      "    \"latent_dim\":128,\n",
      "    \"projection_dim\":30,\n",
      "    \"embed_dim\":1024,\n",
      "    \"num_heads\":4,\n",
      "    \"num_layers\":4,\n",
      "    \"dropout\":0.1,\n",
      "    \"context_size\":50,\n",
      "    \"bias\":True,\n",
      "    \"device\":None,\n",
      "    \"gamma\":0.5,\n",
      "    \"ntx_temp\":0.1,\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "Sane.print_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "50f6ba7f-4b5f-4aaf-bbf3-90cefd1b3460",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "input_dim = 20\n",
    "embed_dim = 64\n",
    "latent_dim = 128\n",
    "num_heads = 4\n",
    "num_layers = 4\n",
    "window_size = 15\n",
    "dropout = 0.1\n",
    "bias = False\n",
    "projection_dim = 10\n",
    "\n",
    "one_epoch_length = 1000\n",
    "epochs = 50 * one_epoch_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "471c307d-c124-47b9-99ff-11c24f5998be",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler_config = {\n",
    "   \n",
    "}\n",
    "\n",
    "Scheduler_params = {\n",
    "                \"scheduler\" : optim.lr_scheduler.OneCycleLR,\n",
    "                \"auto_step\":True,\n",
    "                 \"max_lr\": 3e-4,\n",
    "                \"total_steps\": epochs,\n",
    "                \"pct_start\": 0.3,\n",
    "                \"anneal_strategy\": \"cos\",\n",
    "                \"cycle_momentum\": True,\n",
    "                \"base_momentum\": 0.85,\n",
    "                \"max_momentum\": 0.95,\n",
    "                \"div_factor\": 25.0,\n",
    "                \"final_div_factor\": 10000.0,\n",
    "                \"three_phase\": False,\n",
    "                \"last_epoch\": -1,\n",
    "                \"verbose\": False,\n",
    "}\n",
    "\n",
    "Optimizer_params = {\n",
    "    \"optimizer\":optim.AdamW,\n",
    "    \"optimizer_args\":{\"lr\":3e-4, \"amsgrad\" : True, \"weight_decay\" : 3e-4, \"fused\" : True},\n",
    "    \"clipper\":nn.utils.clip_grad_norm_,\n",
    "    \"clipper_params\":{\"max_norm\" : 5.0},\n",
    "    \"scheduler_params\":Scheduler_params,\n",
    "}\n",
    "\n",
    "Sane_params = {\n",
    "    \"optimizer_params\":Optimizer_params,\n",
    "    \"max_positions\" : [500,500,500],\n",
    "    \"input_dim\":input_dim,\n",
    "    \"latent_dim\":latent_dim,\n",
    "    \"projection_dim\" : projection_dim,\n",
    "    \"embed_dim\":embed_dim,\n",
    "    \"num_heads\":num_heads,\n",
    "    \"num_layers\":num_layers,\n",
    "    \"context_size\":window_size,\n",
    "    \"dropout\":dropout,\n",
    "    \"bias\" : bias,\n",
    "    \"device\":device,\n",
    "    \"gamma\" : 0.5,\n",
    "    \"ntx_temp\" : 0.1,\n",
    "    \"torch_compile\" : False\n",
    "\n",
    "}\n",
    "\n",
    "model = dp.cv.Sane(**Sane_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154e0c35-663b-4d24-b00d-7d28268ee1c1",
   "metadata": {},
   "source": [
    "# Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d56010bb-1512-4837-b2a0-c6e33b252195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assume that layer of a NN is already flattened and the following tensor\n",
      " is batch_size x cout x cr\n",
      "Inp shape : torch.Size([32, 15, 20])\n",
      "mask shape : torch.Size([128, 15, 15])\n",
      "positions shape : torch.Size([32, 15, 3])\n"
     ]
    }
   ],
   "source": [
    "cout = window_size\n",
    "cr = input_dim\n",
    "\n",
    "tokenized_input = torch.rand(size = (batch_size, cout, cr)).to(device)\n",
    "mask = torch.log(torch.randint(0,2,size = (batch_size*num_heads, cout, cout))).to(device)\n",
    "positions = torch.randint(0,500, size = (batch_size,cout,3)).to(device)\n",
    "\n",
    "tokenized_input2 = torch.rand(size = (batch_size, cout, cr)).to(device)\n",
    "mask2 = torch.log(torch.randint(0,2,size = (batch_size*num_heads, cout, cout))).to(device)\n",
    "positions2 = torch.randint(0,500, size = (batch_size,cout,3)).to(device)\n",
    "\n",
    "print(\"Assume that layer of a NN is already flattened and the following tensor\\n is batch_size x cout x cr\")\n",
    "\n",
    "\n",
    "print(f\"Inp shape : {tokenized_input.shape}\")\n",
    "print(f\"mask shape : {mask.shape}\")\n",
    "print(f\"positions shape : {positions.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62627348-6290-4c12-aeec-bdcfdb79a2e9",
   "metadata": {},
   "source": [
    "# Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c90139ec-afc7-440f-a17b-b63487e70a19",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Network(\n",
       "  (model): Sequential(\n",
       "    (0): LinearBeforePosition(\n",
       "      (linear): Linear(in_features=20, out_features=64, bias=True)\n",
       "    )\n",
       "    (1): SanePositionalEmbedding(\n",
       "      (pe1): Embedding(500, 32)\n",
       "      (pe2): Embedding(500, 32)\n",
       "      (pe3): Embedding(500, 32)\n",
       "    )\n",
       "    (2): Dropout(p=0.1, inplace=False)\n",
       "    (3): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-3): 4 x TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=False)\n",
       "          )\n",
       "          (linear1): Linear(in_features=64, out_features=256, bias=False)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=256, out_features=64, bias=False)\n",
       "          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (activation): GELU(approximate='none')\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (4): Linear(in_features=64, out_features=128, bias=True)\n",
       "    (5): LinearBeforePosition(\n",
       "      (linear): Linear(in_features=128, out_features=64, bias=True)\n",
       "    )\n",
       "    (6): SanePositionalEmbedding(\n",
       "      (pe1): Embedding(500, 32)\n",
       "      (pe2): Embedding(500, 32)\n",
       "      (pe3): Embedding(500, 32)\n",
       "    )\n",
       "    (7): Dropout(p=0.1, inplace=False)\n",
       "    (8): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-3): 4 x TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=False)\n",
       "          )\n",
       "          (linear1): Linear(in_features=64, out_features=256, bias=False)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=256, out_features=64, bias=False)\n",
       "          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (activation): GELU(approximate='none')\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (9): Linear(in_features=64, out_features=20, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22e61cc-0262-48ee-8322-b7ab386a3ce1",
   "metadata": {},
   "source": [
    "## Encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fbf9870c-2903-48b3-be88-a1f27dc042a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latent space : torch.Size([32, 15, 128])\n"
     ]
    }
   ],
   "source": [
    "latent = model.encode((tokenized_input,positions))\n",
    "print(f\"Latent space : {latent.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "19bb2c34-7ae7-4bc6-b314-a7de7863fc2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized input : torch.Size([32, 15, 64])\n",
      "Position encoding + dropout : torch.Size([32, 15, 64])\n",
      "After transformer encoder : torch.Size([32, 15, 64])\n",
      "Latent space : torch.Size([32, 15, 128])\n"
     ]
    }
   ],
   "source": [
    "T = model.autoencoder.model[0]((tokenized_input,positions))\n",
    "print(f\"Tokenized input : {T[0].shape}\")\n",
    "\n",
    "T = model.autoencoder.model[1](T)\n",
    "T = model.autoencoder.model[2](T)\n",
    "print(f\"Position encoding + dropout : {T.shape}\")\n",
    "\n",
    "T = model.autoencoder.model[3](T)\n",
    "print(f\"After transformer encoder : {T.shape}\")\n",
    "\n",
    "latent = model.autoencoder.model[4](T)\n",
    "print(f\"Latent space : {latent.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4e4679-9089-439d-838a-db408b6cd952",
   "metadata": {},
   "source": [
    "## Decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3ad68be0-1580-40bd-ba90-2120737a992c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output : torch.Size([32, 15, 20])\n"
     ]
    }
   ],
   "source": [
    "z = model.decode((latent,positions))\n",
    "print(f\"Output : {z.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d09b4233-d45d-484b-af35-1be5d73afd2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder compression : torch.Size([32, 15, 64])\n",
      "Decoder position encoding + dropout : torch.Size([32, 15, 64])\n",
      "Decoder transformer : torch.Size([32, 15, 64])\n",
      "Output : torch.Size([32, 15, 20])\n"
     ]
    }
   ],
   "source": [
    "T = model.autoencoder.model[5]((latent,positions))\n",
    "print(f\"Decoder compression : {T[0].shape}\")\n",
    "\n",
    "T = model.autoencoder.model[6](T)\n",
    "T = model.autoencoder.model[7](T)\n",
    "print(f\"Decoder position encoding + dropout : {T.shape}\")\n",
    "\n",
    "T = model.autoencoder.model[8](T)\n",
    "print(f\"Decoder transformer : {T.shape}\")\n",
    "\n",
    "z = model.autoencoder.model[9](T)\n",
    "print(f\"Output : {z.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d2ec6b-f859-40c1-8171-5b4a1a1dbc63",
   "metadata": {},
   "source": [
    "## Autoencoder Full Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "36a1e326-122c-4b14-b072-c243548f681d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output : torch.Size([32, 15, 20])\n"
     ]
    }
   ],
   "source": [
    "z,y, zp = model((tokenized_input,positions))\n",
    "print(f\"Output : {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a811d5-a94c-4732-928b-c391941a11f5",
   "metadata": {},
   "source": [
    "# Projection Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ae9e8a07-ca60-4bb7-8f71-047511dc2871",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Network(\n",
       "  (model): Sequential(\n",
       "    (0): SqueezeLastDimention()\n",
       "    (1): Linear(in_features=1920, out_features=10, bias=False)\n",
       "    (2): LayerNorm((10,), eps=1e-05, elementwise_affine=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=10, out_features=10, bias=False)\n",
       "    (5): LayerNorm((10,), eps=1e-05, elementwise_affine=True)\n",
       "    (6): ReLU()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a67a25a1-b617-4485-9a06-c7abf472188f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latent size : torch.Size([32, 15, 128])\n",
      "Projection head output size : torch.Size([32, 10])\n"
     ]
    }
   ],
   "source": [
    "latent = model.encode((tokenized_input,positions))\n",
    "p = model.project(latent)\n",
    "print(f\"Latent size : {latent.shape}\")\n",
    "print(f\"Projection head output size : {p.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b5a9b8-d3ca-476e-a132-fb7b33e5bc27",
   "metadata": {},
   "source": [
    "# Train a Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5b4ed7c9-735d-47fa-84ac-d0a69a9456c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss train : 3.614069700241089\n",
      "Loss train : 3.294264078140259\n",
      "Loss train : 3.525251626968384\n",
      "Loss train : 3.455906867980957\n",
      "Loss train : 3.2388103008270264\n",
      "Loss train : 3.193763256072998\n",
      "Loss train : 3.1323351860046387\n",
      "Loss train : 3.2865750789642334\n",
      "Loss train : 3.2153615951538086\n",
      "Loss train : 3.0552492141723633\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    mask = torch.randint(0,2, size = tokenized_input.shape)\n",
    "    mask2 = torch.randint(0,2, size = tokenized_input.shape)\n",
    "    \n",
    "    model.train()\n",
    "    batch = (tokenized_input,positions,mask,tokenized_input2,positions2,mask2)\n",
    "    loss = model.optimize(batch)\n",
    "    print(f\"Loss train : {loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85323026-cec9-4f5f-bd52-6eca95c72674",
   "metadata": {},
   "source": [
    "# Test a Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2ea6dab8-d706-460c-8cec-dd1661dc884c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss train : 3.0454814434051514\n"
     ]
    }
   ],
   "source": [
    "mask = torch.randint(0,2, size = tokenized_input.shape)\n",
    "mask2 = torch.randint(0,2, size = tokenized_input.shape)\n",
    "\n",
    "model.eval()\n",
    "batch = (tokenized_input,positions,mask,tokenized_input2,positions2,mask2)\n",
    "loss = model.test(batch)\n",
    "print(f\"Loss train : {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc58981-780c-48fe-a8ea-2fe08548dd8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
